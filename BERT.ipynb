{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:42.869880Z",
     "iopub.status.busy": "2023-12-19T01:19:42.869497Z",
     "iopub.status.idle": "2023-12-19T01:19:48.393957Z",
     "shell.execute_reply": "2023-12-19T01:19:48.393114Z",
     "shell.execute_reply.started": "2023-12-19T01:19:42.869846Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Device to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:48.396538Z",
     "iopub.status.busy": "2023-12-19T01:19:48.395670Z",
     "iopub.status.idle": "2023-12-19T01:19:48.461555Z",
     "shell.execute_reply": "2023-12-19T01:19:48.460506Z",
     "shell.execute_reply.started": "2023-12-19T01:19:48.396489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:48.463406Z",
     "iopub.status.busy": "2023-12-19T01:19:48.462988Z",
     "iopub.status.idle": "2023-12-19T01:19:54.780439Z",
     "shell.execute_reply": "2023-12-19T01:19:54.779404Z",
     "shell.execute_reply.started": "2023-12-19T01:19:48.463372Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd    \n",
    "dev = pd.read_json(path_or_buf='/kaggle/input/nlp-shared-task-dataset/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "train = pd.read_json(path_or_buf='/kaggle/input/nlp-shared-task-dataset/subtaskA_train_monolingual.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:54.784266Z",
     "iopub.status.busy": "2023-12-19T01:19:54.783577Z",
     "iopub.status.idle": "2023-12-19T01:19:54.915607Z",
     "shell.execute_reply": "2023-12-19T01:19:54.914676Z",
     "shell.execute_reply.started": "2023-12-19T01:19:54.784231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56406 63351 56406 63351\n"
     ]
    }
   ],
   "source": [
    "    train_data_1s=[]\n",
    "    train_data_0s=[]\n",
    "    train_source_1s=[]\n",
    "    train_source_0s=[]\n",
    "    train_text_list=train['text'].tolist()\n",
    "    train_source_list=train['source'].tolist()\n",
    "    train_label_list=train['label'].tolist()\n",
    "    for i in range(len(train)):\n",
    "        if train_label_list[i]==1:\n",
    "            train_data_1s.append(train_text_list[i])\n",
    "            train_source_1s.append(train_source_list[i])\n",
    "        if train_label_list[i]==0:\n",
    "            train_data_0s.append(train_text_list[i])\n",
    "            train_source_0s.append(train_source_list[i])\n",
    "\n",
    "    dev_text_list=dev['text'].tolist()\n",
    "    dev_label_list=dev['label'].tolist()\n",
    "    dev_source_list=dev['source'].tolist()\n",
    "    dev_data_1s=[]\n",
    "    dev_data_0s=[]\n",
    "    dev_source_1s=[]\n",
    "    dev_source_0s=[]\n",
    "    for i in range(len(dev)):\n",
    "        if dev_label_list[i]==1:\n",
    "            dev_data_1s.append(dev_text_list[i])\n",
    "            dev_source_1s.append(dev_source_list[i])\n",
    "        if dev_label_list[i]==0:\n",
    "            dev_data_0s.append(dev_text_list[i])\n",
    "            dev_source_0s.append(dev_source_list[i])\n",
    "    print(len(train_data_1s),len(train_data_0s),len(train_source_1s),len(train_source_0s))\n",
    "    train_1s={'text':train_data_1s,'source':train_source_1s}\n",
    "    train_0s={'text':train_data_0s,'source':train_source_0s}\n",
    "    train_1s = pd.DataFrame(train_1s)\n",
    "    train_0s = pd.DataFrame(train_0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:54.917006Z",
     "iopub.status.busy": "2023-12-19T01:19:54.916734Z",
     "iopub.status.idle": "2023-12-19T01:19:54.942468Z",
     "shell.execute_reply": "2023-12-19T01:19:54.941661Z",
     "shell.execute_reply.started": "2023-12-19T01:19:54.916982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 40000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def retrieveData(records):\n",
    "    train_sample_1s=train_1s.sample(n=records)\n",
    "    train_sample_0s=train_0s.sample(n=records)\n",
    "    train_text=train_sample_1s['text'].tolist()+train_sample_0s['text'].tolist()\n",
    "    train_label=[1 for i in range(records)]+ [0 for i in range(records)]\n",
    "    train_source=train_sample_1s['source'].tolist()+train_sample_0s['source'].tolist()\n",
    "    print(len(train_text),len(train_label),len(train_source))\n",
    "    return train_text,train_label,train_source\n",
    "train_text,train_label,train_source = retrieveData(20000)\n",
    "#dev_text=dev_data_1s[:2000]+dev_data_0s[:2000]\n",
    "#dev_label=[1 for i in range(2000)]+ [0 for i in range(2000)]\n",
    "#dev_source=dev_source_1s[:2000]+dev_source_0s[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:54.943775Z",
     "iopub.status.busy": "2023-12-19T01:19:54.943501Z",
     "iopub.status.idle": "2023-12-19T01:19:54.985506Z",
     "shell.execute_reply": "2023-12-19T01:19:54.984540Z",
     "shell.execute_reply.started": "2023-12-19T01:19:54.943752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000\n"
     ]
    }
   ],
   "source": [
    "train_df={'text':train_text,'source':train_source,'label': train_label}\n",
    "#dev_df={'text':dev_text,'source':dev_source}\n",
    "train_df = pd.DataFrame(train_df)\n",
    "#dev_df = pd.DataFrame(dev_df)\n",
    "train_df=train_df.sample(frac=1)\n",
    "train_label=train_df['label'].tolist()\n",
    "train_df={'text':train_df['text'],'source':train_df['source']}\n",
    "train_df = pd.DataFrame(train_df)\n",
    "print(len(train_df),len(train_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:54.986820Z",
     "iopub.status.busy": "2023-12-19T01:19:54.986546Z",
     "iopub.status.idle": "2023-12-19T01:19:55.004561Z",
     "shell.execute_reply": "2023-12-19T01:19:55.003880Z",
     "shell.execute_reply.started": "2023-12-19T01:19:54.986797Z"
    }
   },
   "outputs": [],
   "source": [
    "#Train Data - 80% Validation Data: 20%\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(train_df, train_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Custom Dataset Class with Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.005968Z",
     "iopub.status.busy": "2023-12-19T01:19:55.005644Z",
     "iopub.status.idle": "2023-12-19T01:19:55.028603Z",
     "shell.execute_reply": "2023-12-19T01:19:55.027669Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.005936Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextAndCategoricalDataset(Dataset):\n",
    "    def __init__(self, texts, sources, labels, tokenizer, tfidf_vectorizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.sources = sources\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tfidf_vectorizer = tfidf_vectorizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def pos_tagging(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        return [tag for (word, tag) in pos_tags]\n",
    "\n",
    "    def named_entity_recognition(self, text):\n",
    "        words = word_tokenize(text)\n",
    "        pos_tags = pos_tag(words)\n",
    "        tree = ne_chunk(pos_tags)\n",
    "        entities = []\n",
    "        for chunk in tree:\n",
    "            if isinstance(chunk, Tree):\n",
    "                entities.append(' '.join([token for token, tag in chunk.leaves()]))\n",
    "        return entities\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        source = str(self.sources[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        \n",
    "        #feature Engineering\n",
    "        # Perform Named Entity Recognition (NER)\n",
    "        #entities = self.named_entity_recognition(text)\n",
    "\n",
    "        # Perform POS tagging\n",
    "        #pos_tags = self.pos_tagging(text)\n",
    "\n",
    "        # Combine text, source, NER, POS tags, and TF-IDF features\n",
    "        #print(entities,pos_tags,self.tfidf_vectorizer.fit_transform([text]))\n",
    "        combined_text = f\"text:{text} [SEP] source:{source}\"\n",
    "\n",
    "        # Tokenize combined text\n",
    "        encoding = self.tokenizer(\n",
    "            combined_text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'encode_text':encoding,\n",
    "            'labels': torch.tensor([label])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating BERT Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.030678Z",
     "iopub.status.busy": "2023-12-19T01:19:55.029922Z",
     "iopub.status.idle": "2023-12-19T01:19:55.041168Z",
     "shell.execute_reply": "2023-12-19T01:19:55.040379Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.030643Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert(**inputs.to(device))\n",
    "        pooled_output = outputs.pooler_output\n",
    "        x = self.dropout(pooled_output)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "        #return torch.argmax(logits, axis=1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.045605Z",
     "iopub.status.busy": "2023-12-19T01:19:55.044961Z",
     "iopub.status.idle": "2023-12-19T01:19:55.062771Z",
     "shell.execute_reply": "2023-12-19T01:19:55.061946Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.045580Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "def train(model, data_loader, optimizer, device,epoch):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = batch['encode_text'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        #inputs = {\n",
    "        #    'input_ids': batch['input_ids'].to(device),\n",
    "        #   'attention_mask': batch['attention_mask'].to(device),\n",
    "        #}\n",
    "        outputs = model(inputs)\n",
    "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    print(f\"epoch {epoch}, loss: {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.064542Z",
     "iopub.status.busy": "2023-12-19T01:19:55.063980Z",
     "iopub.status.idle": "2023-12-19T01:19:55.072254Z",
     "shell.execute_reply": "2023-12-19T01:19:55.071407Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.064508Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "num_classes = 2\n",
    "max_length = 512\n",
    "batch_size = 16\n",
    "num_epochs = 7\n",
    "learning_rate = 0.000009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.073601Z",
     "iopub.status.busy": "2023-12-19T01:19:55.073342Z",
     "iopub.status.idle": "2023-12-19T01:19:55.082385Z",
     "shell.execute_reply": "2023-12-19T01:19:55.081547Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.073578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining evaluation loop\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['encode_text'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _,preds = torch.max(outputs, dim=1)\n",
    "            predictions.extend(preds.to(device).tolist())\n",
    "            actual_labels.extend(labels.to(device).tolist())\n",
    "    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.084005Z",
     "iopub.status.busy": "2023-12-19T01:19:55.083476Z",
     "iopub.status.idle": "2023-12-19T01:19:55.092811Z",
     "shell.execute_reply": "2023-12-19T01:19:55.091992Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.083981Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, tokenizer, device, max_length=512):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "    return \"positive\" if preds.item() == 1 else \"negative\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Model, Optimizer, DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.094091Z",
     "iopub.status.busy": "2023-12-19T01:19:55.093827Z",
     "iopub.status.idle": "2023-12-19T01:19:55.105227Z",
     "shell.execute_reply": "2023-12-19T01:19:55.104463Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.094069Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initializing TF-IFD vectorizer for feature Engineering\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:55.107009Z",
     "iopub.status.busy": "2023-12-19T01:19:55.106167Z",
     "iopub.status.idle": "2023-12-19T01:19:56.013530Z",
     "shell.execute_reply": "2023-12-19T01:19:56.012741Z",
     "shell.execute_reply.started": "2023-12-19T01:19:55.106975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59fb7e83c604ec887c86df29e59dcf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895f16243a31476eaf8045cc5ddba80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299c928c74994cb387ad0068baf30d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7199de3701fe495980a1ac9862ac6b34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initailizing Data Loader\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "train_dataset = TextAndCategoricalDataset(train_data['text'].tolist(),train_data['source'].tolist(), train_labels,tokenizer,tfidf_vectorizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TextAndCategoricalDataset(val_data['text'].tolist(),val_data['source'].tolist(), val_labels,tokenizer,tfidf_vectorizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:19:56.014885Z",
     "iopub.status.busy": "2023-12-19T01:19:56.014599Z",
     "iopub.status.idle": "2023-12-19T01:20:01.741543Z",
     "shell.execute_reply": "2023-12-19T01:20:01.740765Z",
     "shell.execute_reply.started": "2023-12-19T01:19:56.014861Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd18b95ac565405b8c1be0878722c2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intializing BERT Model\n",
    "model = BERTClassifier(bert_model_name, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:20:01.742964Z",
     "iopub.status.busy": "2023-12-19T01:20:01.742682Z",
     "iopub.status.idle": "2023-12-19T01:20:01.748658Z",
     "shell.execute_reply": "2023-12-19T01:20:01.747741Z",
     "shell.execute_reply.started": "2023-12-19T01:20:01.742939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initailizing Adamax Optimiser\n",
    "from torch.optim import Adamax\n",
    "optimizer = torch.optim.Adamax(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T01:20:01.750006Z",
     "iopub.status.busy": "2023-12-19T01:20:01.749745Z",
     "iopub.status.idle": "2023-12-19T11:56:54.986364Z",
     "shell.execute_reply": "2023-12-19T11:56:54.985316Z",
     "shell.execute_reply.started": "2023-12-19T01:20:01.749983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "epoch 1, loss: 0.1787932621262662\n",
      "Validation Accuracy: 0.9091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90      4023\n",
      "           1       0.85      0.99      0.92      3977\n",
      "\n",
      "    accuracy                           0.91      8000\n",
      "   macro avg       0.92      0.91      0.91      8000\n",
      "weighted avg       0.92      0.91      0.91      8000\n",
      "\n",
      "Epoch 2/7\n",
      "epoch 2, loss: 0.08225797993359629\n",
      "Validation Accuracy: 0.9459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94      4023\n",
      "           1       0.91      0.99      0.95      3977\n",
      "\n",
      "    accuracy                           0.95      8000\n",
      "   macro avg       0.95      0.95      0.95      8000\n",
      "weighted avg       0.95      0.95      0.95      8000\n",
      "\n",
      "Epoch 3/7\n",
      "epoch 3, loss: 0.054229181339622304\n",
      "Validation Accuracy: 0.9107\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90      4023\n",
      "           1       0.85      0.99      0.92      3977\n",
      "\n",
      "    accuracy                           0.91      8000\n",
      "   macro avg       0.92      0.91      0.91      8000\n",
      "weighted avg       0.92      0.91      0.91      8000\n",
      "\n",
      "Epoch 4/7\n",
      "epoch 4, loss: 0.0353453561546753\n",
      "Validation Accuracy: 0.9510\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.91      0.95      4023\n",
      "           1       0.92      0.99      0.95      3977\n",
      "\n",
      "    accuracy                           0.95      8000\n",
      "   macro avg       0.95      0.95      0.95      8000\n",
      "weighted avg       0.95      0.95      0.95      8000\n",
      "\n",
      "Epoch 5/7\n",
      "epoch 5, loss: 0.024524911218212764\n",
      "Validation Accuracy: 0.9131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.91      4023\n",
      "           1       0.86      0.99      0.92      3977\n",
      "\n",
      "    accuracy                           0.91      8000\n",
      "   macro avg       0.92      0.91      0.91      8000\n",
      "weighted avg       0.92      0.91      0.91      8000\n",
      "\n",
      "Epoch 6/7\n",
      "epoch 6, loss: 0.017904486042788107\n",
      "Validation Accuracy: 0.9365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      4023\n",
      "           1       0.89      0.99      0.94      3977\n",
      "\n",
      "    accuracy                           0.94      8000\n",
      "   macro avg       0.94      0.94      0.94      8000\n",
      "weighted avg       0.94      0.94      0.94      8000\n",
      "\n",
      "Epoch 7/7\n",
      "epoch 7, loss: 0.013336165232890494\n",
      "Validation Accuracy: 0.9226\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92      4023\n",
      "           1       0.87      0.99      0.93      3977\n",
      "\n",
      "    accuracy                           0.92      8000\n",
      "   macro avg       0.93      0.92      0.92      8000\n",
      "weighted avg       0.93      0.92      0.92      8000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Starting Training\n",
    "for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train(model, train_dataset, optimizer, device,epoch + 1)\n",
    "        accuracy, report = evaluate(model, val_dataset, device)\n",
    "        print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "        print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T11:56:54.987803Z",
     "iopub.status.busy": "2023-12-19T11:56:54.987520Z",
     "iopub.status.idle": "2023-12-19T12:00:38.882988Z",
     "shell.execute_reply": "2023-12-19T12:00:38.881911Z",
     "shell.execute_reply.started": "2023-12-19T11:56:54.987778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Accuracy: 0.8268\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      2500\n",
      "           1       0.82      0.83      0.83      2500\n",
      "\n",
      "    accuracy                           0.83      5000\n",
      "   macro avg       0.83      0.83      0.83      5000\n",
      "weighted avg       0.83      0.83      0.83      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing on Dev dataset\n",
    "dev_text=dev['text'].tolist()\n",
    "dev_label=dev['label'].tolist()\n",
    "dev_source=dev['source'].tolist()\n",
    "dev_dataset = TextAndCategoricalDataset(dev_text,dev_source, dev_label,tokenizer,tfidf_vectorizer)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\n",
    "accuracy, report = evaluate(model, dev_dataset, device)\n",
    "print(f\"Dev Accuracy: {accuracy:.4f}\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4178963,
     "sourceId": 7220264,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4182638,
     "sourceId": 7225307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

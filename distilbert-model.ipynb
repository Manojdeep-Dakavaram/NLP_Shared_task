{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7220264,"sourceType":"datasetVersion","datasetId":4178963},{"sourceId":7225323,"sourceType":"datasetVersion","datasetId":4182649}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Importing Necessary packages","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-12-18T23:49:42.007313Z","iopub.execute_input":"2023-12-18T23:49:42.008189Z","iopub.status.idle":"2023-12-18T23:49:51.690643Z","shell.execute_reply.started":"2023-12-18T23:49:42.008152Z","shell.execute_reply":"2023-12-18T23:49:51.689814Z"}}},{"cell_type":"code","source":"import os\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-12-18T23:49:42.007313Z","iopub.execute_input":"2023-12-18T23:49:42.008189Z","iopub.status.idle":"2023-12-18T23:49:51.690643Z","shell.execute_reply.started":"2023-12-18T23:49:42.008152Z","shell.execute_reply":"2023-12-18T23:49:51.689814Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Importing the datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd    \ndev = pd.read_json(path_or_buf='/kaggle/input/nlp-final-task-dataset/subtaskA_dev_monolingual.jsonl', lines=True)\ntrain = pd.read_json(path_or_buf='/kaggle/input/nlp-final-task-dataset/subtaskA_train_monolingual.jsonl', lines=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:49:55.318312Z","iopub.execute_input":"2023-12-18T23:49:55.318807Z","iopub.status.idle":"2023-12-18T23:50:02.518177Z","shell.execute_reply.started":"2023-12-18T23:49:55.318775Z","shell.execute_reply":"2023-12-18T23:50:02.517011Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing and Shuffling the data","metadata":{}},{"cell_type":"code","source":"train_data_1s=[]\ntrain_data_0s=[]\ntrain_source_1s=[]\ntrain_source_0s=[]\ntrain_text_list=train['text'].tolist()\ntrain_source_list=train['source'].tolist()\ntrain_label_list=train['label'].tolist()\nfor i in range(len(train)):\n    if train_label_list[i]==1:\n        train_data_1s.append(train_text_list[i])\n        train_source_1s.append(train_source_list[i])\n    if train_label_list[i]==0:\n        train_data_0s.append(train_text_list[i])\n        train_source_0s.append(train_source_list[i])\n\ndev_text_list=dev['text'].tolist()\ndev_label_list=dev['label'].tolist()\ndev_source_list=dev['source'].tolist()\ndev_data_1s=[]\ndev_data_0s=[]\ndev_source_1s=[]\ndev_source_0s=[]\nfor i in range(len(dev)):\n    if dev_label_list[i]==1:\n        dev_data_1s.append(dev_text_list[i])\n        dev_source_1s.append(dev_source_list[i])\n    if dev_label_list[i]==0:\n        dev_data_0s.append(dev_text_list[i])\n        dev_source_0s.append(dev_source_list[i])\nprint(len(train_data_1s),len(train_data_0s),len(train_source_1s),len(train_source_0s))\ntrain_1s={'text':train_data_1s,'source':train_source_1s}\ntrain_0s={'text':train_data_0s,'source':train_source_0s}\ntrain_1s = pd.DataFrame(train_1s)\ntrain_0s = pd.DataFrame(train_0s)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:05.401442Z","iopub.execute_input":"2023-12-18T23:50:05.402153Z","iopub.status.idle":"2023-12-18T23:50:05.533866Z","shell.execute_reply.started":"2023-12-18T23:50:05.402119Z","shell.execute_reply":"2023-12-18T23:50:05.532938Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"56406 63351 56406 63351\n","output_type":"stream"}]},{"cell_type":"code","source":"import random\ndef retrieveData(records):\n    train_sample_1s=train_1s.sample(n=records)\n    train_sample_0s=train_0s.sample(n=records)\n    train_text=train_sample_1s['text'].tolist()+train_sample_0s['text'].tolist()\n    train_label=[1 for i in range(records)]+ [0 for i in range(records)]\n    train_source=train_sample_1s['source'].tolist()+train_sample_0s['source'].tolist()\n    print(len(train_text),len(train_label),len(train_source))\n    return train_text,train_label,train_source\ntrain_text,train_label,train_source = retrieveData(20000)\n#dev_text=dev_data_1s[:2000]+dev_data_0s[:2000]\n#dev_label=[1 for i in range(2000)]+ [0 for i in range(2000)]\n#dev_source=dev_source_1s[:2000]+dev_source_0s[:2000]","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:06.028582Z","iopub.execute_input":"2023-12-18T23:50:06.028925Z","iopub.status.idle":"2023-12-18T23:50:06.057131Z","shell.execute_reply.started":"2023-12-18T23:50:06.028882Z","shell.execute_reply":"2023-12-18T23:50:06.056189Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"40000 40000 40000\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df={'text':train_text,'source':train_source,'label': train_label}\n#dev_df={'text':dev_text,'source':dev_source}\ntrain_df = pd.DataFrame(train_df)\n#dev_df = pd.DataFrame(dev_df)\ntrain_df=train_df.sample(frac=1)\ntrain_label=train_df['label'].tolist()\ntrain_df={'text':train_df['text'],'source':train_df['source']}\ntrain_df = pd.DataFrame(train_df)\nprint(len(train_df),len(train_label))","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:08.283519Z","iopub.execute_input":"2023-12-18T23:50:08.283872Z","iopub.status.idle":"2023-12-18T23:50:08.331203Z","shell.execute_reply.started":"2023-12-18T23:50:08.283843Z","shell.execute_reply":"2023-12-18T23:50:08.330278Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"40000 40000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Spliting the train dataset into train and Validation Dataset","metadata":{}},{"cell_type":"code","source":"#Train Dataset - 80% Test Dataset - 20%\ntrain_data, val_data, train_labels, val_labels = train_test_split(train_df, train_label, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:42.532606Z","iopub.execute_input":"2023-12-18T23:50:42.532958Z","iopub.status.idle":"2023-12-18T23:50:42.558584Z","shell.execute_reply.started":"2023-12-18T23:50:42.532928Z","shell.execute_reply":"2023-12-18T23:50:42.557728Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Creating custom Dataset with Feature Engineering","metadata":{}},{"cell_type":"code","source":"class TextAndCategoricalDataset(Dataset):\n    def __init__(self, texts, sources, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.sources = sources\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        source = str(self.sources[idx])\n        label = int(self.labels[idx])\n        \n        #Feature Engineering\n        #punctuation_counts = [text.count(char) for char in \",.!?;:\"]\n        #special_token_count = len([token for token in self.tokenizer.tokenize(text) if token.startswith(\"[\") and token.endswith(\"]\")])\n        #word_count = len(text.split())\n        #word_probs = [text.split().count(word) / word_count for word in set(text.split())]\n        #word_entropy = entropy(word_probs)\n        # Combine text and source information\n        combined_text = f\"text:{text} [SEP] source:{source}\"\n\n        # Tokenize combined text\n        encoding = self.tokenizer(\n            combined_text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initializing Hyper Parameters","metadata":{}},{"cell_type":"code","source":"num_classes = 2\nmax_length = 512\nbatch_size = 16\nnum_epochs = 15\nlearning_rate = 0.000005","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:52.628585Z","iopub.execute_input":"2023-12-18T23:50:52.628956Z","iopub.status.idle":"2023-12-18T23:50:52.633579Z","shell.execute_reply.started":"2023-12-18T23:50:52.628926Z","shell.execute_reply":"2023-12-18T23:50:52.632667Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Defining evaluation loop\ndef evaluate(model, data_loader, device):\n    model.eval()\n    predictions = []\n    actual_labels = []\n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            )\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=1)\n            predictions.extend(preds.cpu().tolist())\n            actual_labels.extend(labels.cpu().tolist())\n    return accuracy_score(actual_labels, predictions), classification_report(actual_labels, predictions)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:53.999672Z","iopub.execute_input":"2023-12-18T23:50:54.000041Z","iopub.status.idle":"2023-12-18T23:50:54.007513Z","shell.execute_reply.started":"2023-12-18T23:50:54.000008Z","shell.execute_reply":"2023-12-18T23:50:54.006487Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Initialize and Train the model","metadata":{}},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\n\n# Tokenize input text\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\ntokenizer.pad_token = tokenizer.cls_token \n#train_tokenized_input = tokenizer.batch_encode_plus(train_text, padding=True, truncation=True, return_tensors='pt')\n#dev_tokenized_input = tokenizer.batch_encode_plus(dev_text, padding=True, truncation=True, return_tensors='pt')\n#train_tokenized_source = tokenizer.batch_encode_plus(train_text, padding=True, truncation=True, return_tensors='pt')\n\n\n# Create Dataset and DataLoader\ntrain_dataset = TextAndCategoricalDataset(train_data['text'].tolist(),train_data['source'].tolist(), train_labels,tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataset = TextAndCategoricalDataset(val_data['text'].tolist(),val_data['source'].tolist(), val_labels,tokenizer)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n\n# Load DistilBERT model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n#model.config.pad_token_id = model.config.cls_token_id\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n#Initilialize Optimizer\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor epoch in range(num_epochs):\n    losses=[]\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    for batch in train_dataloader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        losses.append(loss.item())\n    print(f\"epoch {epoch},loss:{sum(losses)/len(losses)}\")\n    accuracy, report = evaluate(model, val_dataloader, device)\n    print(f\"Validation Accuracy: {accuracy:.4f}\")\n    print(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-18T23:50:54.666754Z","iopub.execute_input":"2023-12-18T23:50:54.667329Z","iopub.status.idle":"2023-12-19T09:21:01.842104Z","shell.execute_reply.started":"2023-12-18T23:50:54.667297Z","shell.execute_reply":"2023-12-19T09:21:01.840620Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"151206a86c4a428d87ec4de3196d0caf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58504ae7cf8742d7889ccd3b634ea27c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64cb4985639f40e3a882636f4a5db12f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3741a9d483a941b7b39ecce71863ef90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd85f30403024a95b0199749248f3161"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/15\nepoch 0,loss:0.17916161352139898\nValidation Accuracy: 0.9621\n              precision    recall  f1-score   support\n\n           0       0.94      0.98      0.96      4003\n           1       0.98      0.94      0.96      3997\n\n    accuracy                           0.96      8000\n   macro avg       0.96      0.96      0.96      8000\nweighted avg       0.96      0.96      0.96      8000\n\nEpoch 2/15\nepoch 1,loss:0.06905059993948089\nValidation Accuracy: 0.9689\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      4003\n           1       0.97      0.96      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 3/15\nepoch 2,loss:0.03695899000222562\nValidation Accuracy: 0.9686\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      4003\n           1       0.97      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 4/15\nepoch 3,loss:0.017377246317337267\nValidation Accuracy: 0.9673\n              precision    recall  f1-score   support\n\n           0       0.98      0.96      0.97      4003\n           1       0.96      0.98      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 5/15\nepoch 4,loss:0.012577247982415428\nValidation Accuracy: 0.9665\n              precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97      4003\n           1       0.98      0.95      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 6/15\nepoch 5,loss:0.00894856075023199\nValidation Accuracy: 0.9684\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97      4003\n           1       0.98      0.96      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 7/15\nepoch 6,loss:0.004477780890567374\nValidation Accuracy: 0.9656\n              precision    recall  f1-score   support\n\n           0       0.97      0.96      0.97      4003\n           1       0.96      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 8/15\nepoch 7,loss:0.006088725748921206\nValidation Accuracy: 0.9700\n              precision    recall  f1-score   support\n\n           0       0.97      0.97      0.97      4003\n           1       0.97      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 9/15\nepoch 8,loss:0.0035025619360640123\nValidation Accuracy: 0.9684\n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97      4003\n           1       0.98      0.95      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 10/15\nepoch 9,loss:0.007034524781549408\nValidation Accuracy: 0.9721\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 11/15\nepoch 10,loss:0.0008430387074195096\nValidation Accuracy: 0.9728\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 12/15\nepoch 11,loss:5.112430560757275e-05\nValidation Accuracy: 0.9722\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 13/15\nepoch 12,loss:1.786117654182817e-05\nValidation Accuracy: 0.9718\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 14/15\nepoch 13,loss:7.478422962435616e-06\nValidation Accuracy: 0.9716\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\nEpoch 15/15\nepoch 14,loss:3.0945890336511184e-06\nValidation Accuracy: 0.9712\n              precision    recall  f1-score   support\n\n           0       0.97      0.98      0.97      4003\n           1       0.98      0.97      0.97      3997\n\n    accuracy                           0.97      8000\n   macro avg       0.97      0.97      0.97      8000\nweighted avg       0.97      0.97      0.97      8000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Testing on Dev Dataset","metadata":{}},{"cell_type":"code","source":"dev_text=dev['text'].tolist()\ndev_label=dev['label'].tolist()\ndev_source=dev['source'].tolist()\ndev_dataset = TextAndCategoricalDataset(dev_text,dev_source, dev_label,tokenizer)\ndev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=True)\naccuracy, report = evaluate(model, dev_dataloader, device)\nprint(f\"Dev Accuracy: {accuracy:.4f}\")\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T09:21:01.844788Z","iopub.execute_input":"2023-12-19T09:21:01.845232Z","iopub.status.idle":"2023-12-19T09:23:40.092720Z","shell.execute_reply.started":"2023-12-19T09:21:01.845193Z","shell.execute_reply":"2023-12-19T09:23:40.091573Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Dev Accuracy: 0.7210\n              precision    recall  f1-score   support\n\n           0       0.65      0.97      0.78      2500\n           1       0.94      0.47      0.63      2500\n\n    accuracy                           0.72      5000\n   macro avg       0.79      0.72      0.70      5000\nweighted avg       0.79      0.72      0.70      5000\n\n","output_type":"stream"}]}]}